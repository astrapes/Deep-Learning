{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=2020\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
       "\n",
       "         9   ...      51      52      53      54      55      56      57  \\\n",
       "0    0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1    0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2    0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3    0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4    0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "203  0.2684  ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
       "204  0.2154  ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
       "205  0.2529  ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
       "206  0.2354  ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
       "207  0.2354  ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
       "\n",
       "         58      59  60  \n",
       "0    0.0090  0.0032   R  \n",
       "1    0.0052  0.0044   R  \n",
       "2    0.0095  0.0078   R  \n",
       "3    0.0040  0.0117   R  \n",
       "4    0.0107  0.0094   R  \n",
       "..      ...     ...  ..  \n",
       "203  0.0193  0.0157   M  \n",
       "204  0.0062  0.0067   M  \n",
       "205  0.0077  0.0031   M  \n",
       "206  0.0036  0.0048   M  \n",
       "207  0.0061  0.0115   M  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/sonar.csv', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "x = np.array(dataset[:,0:60], dtype=np.float64)\n",
    "Y_obj = dataset[:,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = LabelEncoder().fit(Y_obj)\n",
    "y = np.array(e.transform(Y_obj), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['R', 'R', 'R', 'R', 'R'], dtype=object), array([1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_obj[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 24)                1464      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                250       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,725\n",
      "Trainable params: 1,725\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(24, input_dim=60, activation='relu'),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',   # mean_squared_error\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132 samples, validate on 34 samples\n",
      "Epoch 1/200\n",
      "132/132 [==============================] - 0s 3ms/sample - loss: 0.6835 - accuracy: 0.5076 - val_loss: 0.6790 - val_accuracy: 0.5000\n",
      "Epoch 2/200\n",
      "132/132 [==============================] - 0s 208us/sample - loss: 0.6657 - accuracy: 0.6818 - val_loss: 0.6602 - val_accuracy: 0.7353\n",
      "Epoch 3/200\n",
      "132/132 [==============================] - 0s 204us/sample - loss: 0.6512 - accuracy: 0.7424 - val_loss: 0.6468 - val_accuracy: 0.7059\n",
      "Epoch 4/200\n",
      "132/132 [==============================] - 0s 209us/sample - loss: 0.6398 - accuracy: 0.7273 - val_loss: 0.6338 - val_accuracy: 0.7059\n",
      "Epoch 5/200\n",
      "132/132 [==============================] - 0s 201us/sample - loss: 0.6216 - accuracy: 0.7197 - val_loss: 0.6253 - val_accuracy: 0.6471\n",
      "Epoch 6/200\n",
      "132/132 [==============================] - 0s 200us/sample - loss: 0.6044 - accuracy: 0.7348 - val_loss: 0.6022 - val_accuracy: 0.7059\n",
      "Epoch 7/200\n",
      "132/132 [==============================] - 0s 197us/sample - loss: 0.5888 - accuracy: 0.7121 - val_loss: 0.5886 - val_accuracy: 0.7059\n",
      "Epoch 8/200\n",
      "132/132 [==============================] - 0s 210us/sample - loss: 0.5773 - accuracy: 0.7273 - val_loss: 0.5747 - val_accuracy: 0.7353\n",
      "Epoch 9/200\n",
      "132/132 [==============================] - 0s 193us/sample - loss: 0.5592 - accuracy: 0.7424 - val_loss: 0.5656 - val_accuracy: 0.6765\n",
      "Epoch 10/200\n",
      "132/132 [==============================] - 0s 218us/sample - loss: 0.5516 - accuracy: 0.7197 - val_loss: 0.5466 - val_accuracy: 0.7353\n",
      "Epoch 11/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.5420 - accuracy: 0.7424 - val_loss: 0.5398 - val_accuracy: 0.7353\n",
      "Epoch 12/200\n",
      "132/132 [==============================] - 0s 220us/sample - loss: 0.5308 - accuracy: 0.7273 - val_loss: 0.5217 - val_accuracy: 0.7353\n",
      "Epoch 13/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.5158 - accuracy: 0.7727 - val_loss: 0.5052 - val_accuracy: 0.7941\n",
      "Epoch 14/200\n",
      "132/132 [==============================] - 0s 213us/sample - loss: 0.5193 - accuracy: 0.7727 - val_loss: 0.4999 - val_accuracy: 0.7941\n",
      "Epoch 15/200\n",
      "132/132 [==============================] - 0s 207us/sample - loss: 0.5030 - accuracy: 0.7652 - val_loss: 0.4860 - val_accuracy: 0.7941\n",
      "Epoch 16/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.4885 - accuracy: 0.7727 - val_loss: 0.4807 - val_accuracy: 0.7941\n",
      "Epoch 17/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.4719 - accuracy: 0.7652 - val_loss: 0.4633 - val_accuracy: 0.7941\n",
      "Epoch 18/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.4729 - accuracy: 0.7348 - val_loss: 0.4547 - val_accuracy: 0.8235\n",
      "Epoch 19/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.4556 - accuracy: 0.7576 - val_loss: 0.4425 - val_accuracy: 0.7647\n",
      "Epoch 20/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.4502 - accuracy: 0.7652 - val_loss: 0.4326 - val_accuracy: 0.7647\n",
      "Epoch 21/200\n",
      "132/132 [==============================] - 0s 213us/sample - loss: 0.4456 - accuracy: 0.7879 - val_loss: 0.4278 - val_accuracy: 0.7647\n",
      "Epoch 22/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.4318 - accuracy: 0.7955 - val_loss: 0.4230 - val_accuracy: 0.8529\n",
      "Epoch 23/200\n",
      "132/132 [==============================] - 0s 231us/sample - loss: 0.4250 - accuracy: 0.8106 - val_loss: 0.4090 - val_accuracy: 0.7941\n",
      "Epoch 24/200\n",
      "132/132 [==============================] - 0s 211us/sample - loss: 0.4142 - accuracy: 0.7955 - val_loss: 0.4086 - val_accuracy: 0.8529\n",
      "Epoch 25/200\n",
      "132/132 [==============================] - 0s 220us/sample - loss: 0.4154 - accuracy: 0.8106 - val_loss: 0.3927 - val_accuracy: 0.7647\n",
      "Epoch 26/200\n",
      "132/132 [==============================] - 0s 212us/sample - loss: 0.4075 - accuracy: 0.8333 - val_loss: 0.3830 - val_accuracy: 0.7941\n",
      "Epoch 27/200\n",
      "132/132 [==============================] - 0s 205us/sample - loss: 0.3952 - accuracy: 0.8030 - val_loss: 0.3841 - val_accuracy: 0.8235\n",
      "Epoch 28/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.3914 - accuracy: 0.8333 - val_loss: 0.3755 - val_accuracy: 0.8235\n",
      "Epoch 29/200\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.5581 - accuracy: 0.60 - 0s 226us/sample - loss: 0.3861 - accuracy: 0.7955 - val_loss: 0.3722 - val_accuracy: 0.7941\n",
      "Epoch 30/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.3768 - accuracy: 0.8258 - val_loss: 0.3764 - val_accuracy: 0.7647\n",
      "Epoch 31/200\n",
      "132/132 [==============================] - 0s 225us/sample - loss: 0.3742 - accuracy: 0.8182 - val_loss: 0.3536 - val_accuracy: 0.8235\n",
      "Epoch 32/200\n",
      "132/132 [==============================] - 0s 218us/sample - loss: 0.3746 - accuracy: 0.8030 - val_loss: 0.3513 - val_accuracy: 0.8529\n",
      "Epoch 33/200\n",
      "132/132 [==============================] - 0s 210us/sample - loss: 0.3529 - accuracy: 0.8636 - val_loss: 0.3485 - val_accuracy: 0.7941\n",
      "Epoch 34/200\n",
      "132/132 [==============================] - 0s 229us/sample - loss: 0.3476 - accuracy: 0.8636 - val_loss: 0.3516 - val_accuracy: 0.7941\n",
      "Epoch 35/200\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 0.3455 - accuracy: 0.8864 - val_loss: 0.3360 - val_accuracy: 0.8529\n",
      "Epoch 36/200\n",
      "132/132 [==============================] - 0s 218us/sample - loss: 0.3366 - accuracy: 0.8561 - val_loss: 0.3233 - val_accuracy: 0.8529\n",
      "Epoch 37/200\n",
      "132/132 [==============================] - 0s 232us/sample - loss: 0.3209 - accuracy: 0.9091 - val_loss: 0.3242 - val_accuracy: 0.8824\n",
      "Epoch 38/200\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 0.3172 - accuracy: 0.8939 - val_loss: 0.3206 - val_accuracy: 0.8529\n",
      "Epoch 39/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.3113 - accuracy: 0.8939 - val_loss: 0.3148 - val_accuracy: 0.8824\n",
      "Epoch 40/200\n",
      "132/132 [==============================] - 0s 214us/sample - loss: 0.3121 - accuracy: 0.8864 - val_loss: 0.3142 - val_accuracy: 0.8824\n",
      "Epoch 41/200\n",
      "132/132 [==============================] - 0s 218us/sample - loss: 0.2984 - accuracy: 0.9091 - val_loss: 0.3079 - val_accuracy: 0.8824\n",
      "Epoch 42/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.2924 - accuracy: 0.9015 - val_loss: 0.3084 - val_accuracy: 0.8824\n",
      "Epoch 43/200\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 0.2832 - accuracy: 0.9167 - val_loss: 0.3054 - val_accuracy: 0.8529\n",
      "Epoch 44/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.2772 - accuracy: 0.9167 - val_loss: 0.2967 - val_accuracy: 0.8824\n",
      "Epoch 45/200\n",
      "132/132 [==============================] - 0s 211us/sample - loss: 0.2713 - accuracy: 0.9318 - val_loss: 0.2904 - val_accuracy: 0.8824\n",
      "Epoch 46/200\n",
      "132/132 [==============================] - 0s 218us/sample - loss: 0.2682 - accuracy: 0.9318 - val_loss: 0.3053 - val_accuracy: 0.8529\n",
      "Epoch 47/200\n",
      "132/132 [==============================] - 0s 212us/sample - loss: 0.2619 - accuracy: 0.9091 - val_loss: 0.2935 - val_accuracy: 0.8824\n",
      "Epoch 48/200\n",
      "132/132 [==============================] - 0s 232us/sample - loss: 0.2543 - accuracy: 0.9394 - val_loss: 0.2868 - val_accuracy: 0.8824\n",
      "Epoch 49/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.2497 - accuracy: 0.9318 - val_loss: 0.2809 - val_accuracy: 0.8824\n",
      "Epoch 50/200\n",
      "132/132 [==============================] - 0s 211us/sample - loss: 0.2478 - accuracy: 0.9470 - val_loss: 0.2763 - val_accuracy: 0.8235\n",
      "Epoch 51/200\n",
      "132/132 [==============================] - 0s 216us/sample - loss: 0.2408 - accuracy: 0.9318 - val_loss: 0.2918 - val_accuracy: 0.8824\n",
      "Epoch 52/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.2420 - accuracy: 0.9318 - val_loss: 0.2759 - val_accuracy: 0.9118\n",
      "Epoch 53/200\n",
      "132/132 [==============================] - 0s 212us/sample - loss: 0.2352 - accuracy: 0.9318 - val_loss: 0.2714 - val_accuracy: 0.9118\n",
      "Epoch 54/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.2290 - accuracy: 0.9470 - val_loss: 0.2810 - val_accuracy: 0.8824\n",
      "Epoch 55/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.2221 - accuracy: 0.9394 - val_loss: 0.2722 - val_accuracy: 0.8235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/200\n",
      "132/132 [==============================] - 0s 225us/sample - loss: 0.2202 - accuracy: 0.9394 - val_loss: 0.2730 - val_accuracy: 0.9118\n",
      "Epoch 57/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.2333 - accuracy: 0.9242 - val_loss: 0.2702 - val_accuracy: 0.8529\n",
      "Epoch 58/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.2202 - accuracy: 0.9394 - val_loss: 0.2966 - val_accuracy: 0.8529\n",
      "Epoch 59/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.2233 - accuracy: 0.9394 - val_loss: 0.2683 - val_accuracy: 0.8824\n",
      "Epoch 60/200\n",
      "132/132 [==============================] - 0s 210us/sample - loss: 0.2078 - accuracy: 0.9318 - val_loss: 0.2707 - val_accuracy: 0.8529\n",
      "Epoch 61/200\n",
      "132/132 [==============================] - 0s 213us/sample - loss: 0.2041 - accuracy: 0.9318 - val_loss: 0.3025 - val_accuracy: 0.8235\n",
      "Epoch 62/200\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 0.2111 - accuracy: 0.9318 - val_loss: 0.2761 - val_accuracy: 0.8529\n",
      "Epoch 63/200\n",
      "132/132 [==============================] - 0s 220us/sample - loss: 0.1960 - accuracy: 0.9470 - val_loss: 0.2581 - val_accuracy: 0.8235\n",
      "Epoch 64/200\n",
      "132/132 [==============================] - 0s 213us/sample - loss: 0.1899 - accuracy: 0.9545 - val_loss: 0.2770 - val_accuracy: 0.8824\n",
      "Epoch 65/200\n",
      "132/132 [==============================] - 0s 218us/sample - loss: 0.2070 - accuracy: 0.9545 - val_loss: 0.2550 - val_accuracy: 0.8824\n",
      "Epoch 66/200\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 0.1913 - accuracy: 0.9470 - val_loss: 0.2598 - val_accuracy: 0.8824\n",
      "Epoch 67/200\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 0.1814 - accuracy: 0.9545 - val_loss: 0.2594 - val_accuracy: 0.8824\n",
      "Epoch 68/200\n",
      "132/132 [==============================] - 0s 207us/sample - loss: 0.1756 - accuracy: 0.9318 - val_loss: 0.2564 - val_accuracy: 0.8529\n",
      "Epoch 69/200\n",
      "132/132 [==============================] - 0s 216us/sample - loss: 0.1705 - accuracy: 0.9545 - val_loss: 0.2564 - val_accuracy: 0.8824\n",
      "Epoch 70/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.1654 - accuracy: 0.9545 - val_loss: 0.2501 - val_accuracy: 0.8824\n",
      "Epoch 71/200\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 0.1635 - accuracy: 0.9621 - val_loss: 0.2588 - val_accuracy: 0.8824\n",
      "Epoch 72/200\n",
      "132/132 [==============================] - 0s 214us/sample - loss: 0.1602 - accuracy: 0.9470 - val_loss: 0.2700 - val_accuracy: 0.8235\n",
      "Epoch 73/200\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 1.00 - 0s 219us/sample - loss: 0.1572 - accuracy: 0.9545 - val_loss: 0.2535 - val_accuracy: 0.8824\n",
      "Epoch 74/200\n",
      "132/132 [==============================] - 0s 222us/sample - loss: 0.1514 - accuracy: 0.9621 - val_loss: 0.2497 - val_accuracy: 0.8824\n",
      "Epoch 75/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.1533 - accuracy: 0.9545 - val_loss: 0.2502 - val_accuracy: 0.8824\n",
      "Epoch 76/200\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 0.1503 - accuracy: 0.9621 - val_loss: 0.2518 - val_accuracy: 0.8824\n",
      "Epoch 77/200\n",
      "132/132 [==============================] - 0s 206us/sample - loss: 0.1595 - accuracy: 0.9394 - val_loss: 0.3289 - val_accuracy: 0.8235\n",
      "Epoch 78/200\n",
      "132/132 [==============================] - 0s 216us/sample - loss: 0.1570 - accuracy: 0.9545 - val_loss: 0.2701 - val_accuracy: 0.8824\n",
      "Epoch 79/200\n",
      "132/132 [==============================] - 0s 218us/sample - loss: 0.1448 - accuracy: 0.9545 - val_loss: 0.2849 - val_accuracy: 0.8824\n",
      "Epoch 80/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.1509 - accuracy: 0.9394 - val_loss: 0.2555 - val_accuracy: 0.8824\n",
      "Epoch 81/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.1463 - accuracy: 0.9470 - val_loss: 0.2613 - val_accuracy: 0.9118\n",
      "Epoch 82/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.1384 - accuracy: 0.9697 - val_loss: 0.2503 - val_accuracy: 0.9118\n",
      "Epoch 83/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.1363 - accuracy: 0.9545 - val_loss: 0.2769 - val_accuracy: 0.8824\n",
      "Epoch 84/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.1280 - accuracy: 0.9697 - val_loss: 0.2506 - val_accuracy: 0.8824\n",
      "Epoch 85/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.1253 - accuracy: 0.9697 - val_loss: 0.2553 - val_accuracy: 0.8824\n",
      "Epoch 86/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.1278 - accuracy: 0.9697 - val_loss: 0.2689 - val_accuracy: 0.8529\n",
      "Epoch 87/200\n",
      "132/132 [==============================] - 0s 209us/sample - loss: 0.1244 - accuracy: 0.9697 - val_loss: 0.2595 - val_accuracy: 0.9118\n",
      "Epoch 88/200\n",
      "132/132 [==============================] - 0s 213us/sample - loss: 0.1225 - accuracy: 0.9621 - val_loss: 0.2638 - val_accuracy: 0.8529\n",
      "Epoch 89/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.1211 - accuracy: 0.9697 - val_loss: 0.2655 - val_accuracy: 0.8824\n",
      "Epoch 90/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.1143 - accuracy: 0.9697 - val_loss: 0.2694 - val_accuracy: 0.8824\n",
      "Epoch 91/200\n",
      "132/132 [==============================] - 0s 215us/sample - loss: 0.1114 - accuracy: 0.9697 - val_loss: 0.2588 - val_accuracy: 0.8824\n",
      "Epoch 92/200\n",
      "132/132 [==============================] - 0s 216us/sample - loss: 0.1113 - accuracy: 0.9697 - val_loss: 0.2558 - val_accuracy: 0.9118\n",
      "Epoch 93/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.1037 - accuracy: 0.9697 - val_loss: 0.2515 - val_accuracy: 0.9118\n",
      "Epoch 94/200\n",
      "132/132 [==============================] - 0s 220us/sample - loss: 0.1082 - accuracy: 0.9697 - val_loss: 0.2485 - val_accuracy: 0.9118\n",
      "Epoch 95/200\n",
      "132/132 [==============================] - 0s 225us/sample - loss: 0.0978 - accuracy: 0.9697 - val_loss: 0.2438 - val_accuracy: 0.8824\n",
      "Epoch 96/200\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 0.0999 - accuracy: 0.9697 - val_loss: 0.2565 - val_accuracy: 0.9118\n",
      "Epoch 97/200\n",
      "132/132 [==============================] - 0s 193us/sample - loss: 0.0976 - accuracy: 0.9697 - val_loss: 0.2495 - val_accuracy: 0.9118\n",
      "Epoch 98/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.0973 - accuracy: 0.9697 - val_loss: 0.2632 - val_accuracy: 0.8824\n",
      "Epoch 99/200\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 0.1075 - accuracy: 0.9621 - val_loss: 0.2639 - val_accuracy: 0.9118\n",
      "Epoch 100/200\n",
      "132/132 [==============================] - 0s 218us/sample - loss: 0.0896 - accuracy: 0.9697 - val_loss: 0.2501 - val_accuracy: 0.8824\n",
      "Epoch 101/200\n",
      "132/132 [==============================] - 0s 214us/sample - loss: 0.0925 - accuracy: 0.9773 - val_loss: 0.2528 - val_accuracy: 0.9118\n",
      "Epoch 102/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.0908 - accuracy: 0.9697 - val_loss: 0.2458 - val_accuracy: 0.9118\n",
      "Epoch 103/200\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 0.0855 - accuracy: 0.9697 - val_loss: 0.2555 - val_accuracy: 0.9118\n",
      "Epoch 104/200\n",
      "132/132 [==============================] - 0s 220us/sample - loss: 0.0845 - accuracy: 0.9697 - val_loss: 0.2495 - val_accuracy: 0.9118\n",
      "Epoch 105/200\n",
      "132/132 [==============================] - 0s 228us/sample - loss: 0.0795 - accuracy: 0.9773 - val_loss: 0.2641 - val_accuracy: 0.9118\n",
      "Epoch 106/200\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 1.00 - 0s 205us/sample - loss: 0.0821 - accuracy: 0.9621 - val_loss: 0.2627 - val_accuracy: 0.9118\n",
      "Epoch 107/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.0800 - accuracy: 0.9697 - val_loss: 0.2512 - val_accuracy: 0.9118\n",
      "Epoch 108/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.0768 - accuracy: 0.9697 - val_loss: 0.2635 - val_accuracy: 0.9118\n",
      "Epoch 109/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.0931 - accuracy: 0.9773 - val_loss: 0.2603 - val_accuracy: 0.9118\n",
      "Epoch 110/200\n",
      "132/132 [==============================] - 0s 229us/sample - loss: 0.0898 - accuracy: 0.9697 - val_loss: 0.2715 - val_accuracy: 0.9118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.0682 - accuracy: 0.9773 - val_loss: 0.2724 - val_accuracy: 0.9118\n",
      "Epoch 112/200\n",
      "132/132 [==============================] - 0s 229us/sample - loss: 0.0747 - accuracy: 0.9848 - val_loss: 0.2558 - val_accuracy: 0.9118\n",
      "Epoch 113/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.0662 - accuracy: 0.9848 - val_loss: 0.2516 - val_accuracy: 0.9118\n",
      "Epoch 114/200\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 0.0659 - accuracy: 0.9848 - val_loss: 0.2595 - val_accuracy: 0.9118\n",
      "Epoch 115/200\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 0.0692 - accuracy: 1.0000 - val_loss: 0.2788 - val_accuracy: 0.8824\n",
      "Epoch 116/200\n",
      "132/132 [==============================] - 0s 228us/sample - loss: 0.0659 - accuracy: 0.9773 - val_loss: 0.2522 - val_accuracy: 0.9118\n",
      "Epoch 117/200\n",
      "132/132 [==============================] - 0s 213us/sample - loss: 0.0611 - accuracy: 0.9848 - val_loss: 0.2540 - val_accuracy: 0.9118\n",
      "Epoch 118/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.0736 - accuracy: 0.9924 - val_loss: 0.2769 - val_accuracy: 0.8824\n",
      "Epoch 119/200\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 0.0576 - accuracy: 0.9848 - val_loss: 0.2681 - val_accuracy: 0.8824\n",
      "Epoch 120/200\n",
      "132/132 [==============================] - 0s 220us/sample - loss: 0.0585 - accuracy: 0.9924 - val_loss: 0.2619 - val_accuracy: 0.9118\n",
      "Epoch 121/200\n",
      "132/132 [==============================] - 0s 229us/sample - loss: 0.0600 - accuracy: 0.9924 - val_loss: 0.2762 - val_accuracy: 0.8824\n",
      "Epoch 122/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.0534 - accuracy: 0.9924 - val_loss: 0.2641 - val_accuracy: 0.8824\n",
      "Epoch 123/200\n",
      "132/132 [==============================] - 0s 225us/sample - loss: 0.0537 - accuracy: 0.9848 - val_loss: 0.2555 - val_accuracy: 0.9118\n",
      "Epoch 124/200\n",
      "132/132 [==============================] - 0s 222us/sample - loss: 0.0531 - accuracy: 0.9924 - val_loss: 0.2598 - val_accuracy: 0.9118\n",
      "Epoch 125/200\n",
      "132/132 [==============================] - 0s 230us/sample - loss: 0.0500 - accuracy: 1.0000 - val_loss: 0.2495 - val_accuracy: 0.9118\n",
      "Epoch 126/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.0506 - accuracy: 0.9924 - val_loss: 0.2534 - val_accuracy: 0.9118\n",
      "Epoch 127/200\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 1.00 - 0s 236us/sample - loss: 0.0486 - accuracy: 1.0000 - val_loss: 0.2645 - val_accuracy: 0.9118\n",
      "Epoch 128/200\n",
      "132/132 [==============================] - 0s 215us/sample - loss: 0.0447 - accuracy: 0.9924 - val_loss: 0.2666 - val_accuracy: 0.9118\n",
      "Epoch 129/200\n",
      "132/132 [==============================] - 0s 225us/sample - loss: 0.0490 - accuracy: 0.9924 - val_loss: 0.2771 - val_accuracy: 0.9118\n",
      "Epoch 130/200\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 0.0457 - accuracy: 0.9924 - val_loss: 0.2675 - val_accuracy: 0.9118\n",
      "Epoch 131/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.0430 - accuracy: 1.0000 - val_loss: 0.2593 - val_accuracy: 0.9118\n",
      "Epoch 132/200\n",
      "132/132 [==============================] - 0s 213us/sample - loss: 0.0416 - accuracy: 0.9924 - val_loss: 0.2605 - val_accuracy: 0.9118\n",
      "Epoch 133/200\n",
      "132/132 [==============================] - 0s 228us/sample - loss: 0.0478 - accuracy: 1.0000 - val_loss: 0.2831 - val_accuracy: 0.9118\n",
      "Epoch 134/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.0399 - accuracy: 1.0000 - val_loss: 0.2889 - val_accuracy: 0.8529\n",
      "Epoch 135/200\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 0.0445 - accuracy: 0.9924 - val_loss: 0.2774 - val_accuracy: 0.8824\n",
      "Epoch 136/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.0416 - accuracy: 1.0000 - val_loss: 0.2765 - val_accuracy: 0.8824\n",
      "Epoch 137/200\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 1.00 - 0s 218us/sample - loss: 0.0373 - accuracy: 1.0000 - val_loss: 0.2621 - val_accuracy: 0.9118\n",
      "Epoch 138/200\n",
      "132/132 [==============================] - 0s 208us/sample - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.2651 - val_accuracy: 0.8824\n",
      "Epoch 139/200\n",
      "132/132 [==============================] - 0s 222us/sample - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.2630 - val_accuracy: 0.9118\n",
      "Epoch 140/200\n",
      "132/132 [==============================] - 0s 215us/sample - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.2587 - val_accuracy: 0.9118\n",
      "Epoch 141/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.2653 - val_accuracy: 0.9118\n",
      "Epoch 142/200\n",
      "132/132 [==============================] - 0s 222us/sample - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.2672 - val_accuracy: 0.9118\n",
      "Epoch 143/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.2773 - val_accuracy: 0.9118\n",
      "Epoch 144/200\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 0.0325 - accuracy: 1.0000 - val_loss: 0.2611 - val_accuracy: 0.9118\n",
      "Epoch 145/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.0299 - accuracy: 1.0000 - val_loss: 0.2638 - val_accuracy: 0.9118\n",
      "Epoch 146/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.2712 - val_accuracy: 0.8824\n",
      "Epoch 147/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.0307 - accuracy: 1.0000 - val_loss: 0.2554 - val_accuracy: 0.8824\n",
      "Epoch 148/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.2785 - val_accuracy: 0.9118\n",
      "Epoch 149/200\n",
      "132/132 [==============================] - 0s 228us/sample - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.2817 - val_accuracy: 0.8824\n",
      "Epoch 150/200\n",
      "132/132 [==============================] - 0s 228us/sample - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.2749 - val_accuracy: 0.9118\n",
      "Epoch 151/200\n",
      "132/132 [==============================] - 0s 228us/sample - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.2819 - val_accuracy: 0.8824\n",
      "Epoch 152/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.2728 - val_accuracy: 0.9118\n",
      "Epoch 153/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.2726 - val_accuracy: 0.9118\n",
      "Epoch 154/200\n",
      "132/132 [==============================] - 0s 225us/sample - loss: 0.0249 - accuracy: 1.0000 - val_loss: 0.2751 - val_accuracy: 0.9118\n",
      "Epoch 155/200\n",
      "132/132 [==============================] - 0s 231us/sample - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.2718 - val_accuracy: 0.9118\n",
      "Epoch 156/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.2836 - val_accuracy: 0.8824\n",
      "Epoch 157/200\n",
      "132/132 [==============================] - 0s 228us/sample - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.2623 - val_accuracy: 0.8824\n",
      "Epoch 158/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.0218 - accuracy: 1.0000 - val_loss: 0.2709 - val_accuracy: 0.8824\n",
      "Epoch 159/200\n",
      "132/132 [==============================] - 0s 212us/sample - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.2682 - val_accuracy: 0.9118\n",
      "Epoch 160/200\n",
      "132/132 [==============================] - 0s 213us/sample - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.2727 - val_accuracy: 0.9118\n",
      "Epoch 161/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.2749 - val_accuracy: 0.8824\n",
      "Epoch 162/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.2795 - val_accuracy: 0.9118\n",
      "Epoch 163/200\n",
      "132/132 [==============================] - 0s 212us/sample - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.2701 - val_accuracy: 0.8824\n",
      "Epoch 164/200\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.2777 - val_accuracy: 0.8824\n",
      "Epoch 165/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 0s 210us/sample - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.2851 - val_accuracy: 0.9118\n",
      "Epoch 166/200\n",
      "132/132 [==============================] - 0s 220us/sample - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.2708 - val_accuracy: 0.9118\n",
      "Epoch 167/200\n",
      "132/132 [==============================] - 0s 218us/sample - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.2822 - val_accuracy: 0.8824\n",
      "Epoch 168/200\n",
      "132/132 [==============================] - 0s 228us/sample - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.2880 - val_accuracy: 0.9118\n",
      "Epoch 169/200\n",
      "132/132 [==============================] - 0s 225us/sample - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.2847 - val_accuracy: 0.8824\n",
      "Epoch 170/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.2856 - val_accuracy: 0.9118\n",
      "Epoch 171/200\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.2770 - val_accuracy: 0.8824\n",
      "Epoch 172/200\n",
      "132/132 [==============================] - 0s 228us/sample - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.2876 - val_accuracy: 0.8824\n",
      "Epoch 173/200\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.2843 - val_accuracy: 0.8824\n",
      "Epoch 174/200\n",
      "132/132 [==============================] - 0s 223us/sample - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.2895 - val_accuracy: 0.8824\n",
      "Epoch 175/200\n",
      "132/132 [==============================] - 0s 225us/sample - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.2916 - val_accuracy: 0.8824\n",
      "Epoch 176/200\n",
      "132/132 [==============================] - 0s 214us/sample - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.2854 - val_accuracy: 0.8824\n",
      "Epoch 177/200\n",
      "132/132 [==============================] - 0s 220us/sample - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.2883 - val_accuracy: 0.8824\n",
      "Epoch 178/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.2958 - val_accuracy: 0.9118\n",
      "Epoch 179/200\n",
      "132/132 [==============================] - 0s 228us/sample - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.2972 - val_accuracy: 0.9118\n",
      "Epoch 180/200\n",
      "132/132 [==============================] - 0s 213us/sample - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.2859 - val_accuracy: 0.8824\n",
      "Epoch 181/200\n",
      "132/132 [==============================] - 0s 209us/sample - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.2843 - val_accuracy: 0.9118\n",
      "Epoch 182/200\n",
      "132/132 [==============================] - 0s 222us/sample - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.2996 - val_accuracy: 0.9118\n",
      "Epoch 183/200\n",
      "132/132 [==============================] - 0s 239us/sample - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.2955 - val_accuracy: 0.9118\n",
      "Epoch 184/200\n",
      "132/132 [==============================] - 0s 219us/sample - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.2832 - val_accuracy: 0.8824\n",
      "Epoch 185/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.2940 - val_accuracy: 0.8824\n",
      "Epoch 186/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.3043 - val_accuracy: 0.8824\n",
      "Epoch 187/200\n",
      "132/132 [==============================] - 0s 227us/sample - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.2999 - val_accuracy: 0.8824\n",
      "Epoch 188/200\n",
      "132/132 [==============================] - 0s 229us/sample - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.2923 - val_accuracy: 0.8824\n",
      "Epoch 189/200\n",
      "132/132 [==============================] - 0s 229us/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.2975 - val_accuracy: 0.8824\n",
      "Epoch 190/200\n",
      "132/132 [==============================] - 0s 224us/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3014 - val_accuracy: 0.8824\n",
      "Epoch 191/200\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.2971 - val_accuracy: 0.8824\n",
      "Epoch 192/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.3010 - val_accuracy: 0.8824\n",
      "Epoch 193/200\n",
      "132/132 [==============================] - 0s 231us/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.3111 - val_accuracy: 0.8824\n",
      "Epoch 194/200\n",
      "132/132 [==============================] - 0s 222us/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3025 - val_accuracy: 0.8824\n",
      "Epoch 195/200\n",
      "132/132 [==============================] - 0s 209us/sample - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.3022 - val_accuracy: 0.8824\n",
      "Epoch 196/200\n",
      "132/132 [==============================] - 0s 221us/sample - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3198 - val_accuracy: 0.9118\n",
      "Epoch 197/200\n",
      "132/132 [==============================] - 0s 226us/sample - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.3119 - val_accuracy: 0.8824\n",
      "Epoch 198/200\n",
      "132/132 [==============================] - 0s 222us/sample - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.3015 - val_accuracy: 0.8824\n",
      "Epoch 199/200\n",
      "132/132 [==============================] - 0s 217us/sample - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.3075 - val_accuracy: 0.8824\n",
      "Epoch 200/200\n",
      "132/132 [==============================] - 0s 229us/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.3101 - val_accuracy: 0.8824\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train, y_train, validation_split=0.2, epochs=200, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 - 0s - loss: 0.8734 - accuracy: 0.8095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8095238"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.683513</td>\n",
       "      <td>0.507576</td>\n",
       "      <td>0.678985</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.665704</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.660157</td>\n",
       "      <td>0.735294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.651247</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>0.646810</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.639819</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.633759</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.621570</td>\n",
       "      <td>0.719697</td>\n",
       "      <td>0.625250</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.008643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.319816</td>\n",
       "      <td>0.911765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.008544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.311886</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.007770</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.301508</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.007480</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.307543</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.007324</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.310051</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy  val_loss  val_accuracy\n",
       "0    0.683513  0.507576  0.678985      0.500000\n",
       "1    0.665704  0.681818  0.660157      0.735294\n",
       "2    0.651247  0.742424  0.646810      0.705882\n",
       "3    0.639819  0.727273  0.633759      0.705882\n",
       "4    0.621570  0.719697  0.625250      0.647059\n",
       "..        ...       ...       ...           ...\n",
       "195  0.008643  1.000000  0.319816      0.911765\n",
       "196  0.008544  1.000000  0.311886      0.882353\n",
       "197  0.007770  1.000000  0.301508      0.882353\n",
       "198  0.007480  1.000000  0.307543      0.882353\n",
       "199  0.007324  1.000000  0.310051      0.882353\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(history.history)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1761e801748>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df.index,df[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
